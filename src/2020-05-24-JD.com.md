---
layout: post
title: 京东全网爬虫
slug: 2020-05-24-JD.com
date: 2020-5-24 16:20
status: publish
author: Kanshan
categories: 
  - Python
  - 爬虫
tags:
  - 京东全网爬虫
excerpt: 获取京东网站商品信息
---



#### 1.需求

- 目标：明确要抓取哪些信息

##### 1.1抓取首页的分类信息

- 抓取数据：各级分类的名称和 URL

  ![](./images/2020-5-24-1.png)

##### 1.2 抓取商品信息

- 抓取：商品名称，商品价格，商品评论数，商品店铺，商品促销，商品选项，商品图片的URL

  ![](.\images\2020-5-24-2.png)



#### 2.开发环境

- 平台：Windows 也可以运行在Mac 和Linux
- 开发语言：Python
- 开发工具：Pycharm
- 技术选择：
  - 由于全网爬虫，抓取页面非常多，为了提高抓的速度，选择使用scrapy框架+scrapy_redis分布式组件
  - 由于京东全网的数据量达到了亿级，存储又是结构化数据，数据库选择使用MongoDB



####  3.京东全网爬虫的实现步骤

> 我们采用广度优先策略，把分类和商品信息分开来做
>
> 好处：可以提高程序的稳定性

##### 	3.1 总体设计

![](.\images\2020-5-24-3.png)

##### 	3.2 实现步骤

> 1.创建爬虫项目
>
> 2.根据需求，定义数据模型
>
> 3.实现分类爬虫
>
> 4.保存分类信息
>
> 5.实现商品爬虫
>
> 6.保存商品信息
>
> 7.实现随机User-Agent和代理IP下载器中间件，解决IP反爬

##### 	3.3创建爬虫项目

- ```scrapy startproject mall_spider```



#### 4. 明确要抓取的数据（定义数据模型）

> 爬虫数据模型，我们只能根据需求，定义一个大概，随着对项目的实现可能会对数据模型作相应的修改

##### 	4.1 类别数据模型

- 类别数据模型类：用于存储类别信息（Category）- 字段：
  - `b_category_name`：大类别名称
  - `b_category_url`：大类别URL
  - `m_category_name`：中分类名称
  - `m_category_url`：中分类URL
  - `s_category_name`：小分类名称
  - `s_category_url`：小分类URL
- 代码：

```python
class Category(scrapy.Item):
    """商品类别"""
    #大分类名称
    b_category_name = scrapy.Field()
    #大分类URL
    b_category_url = scrapy.Field()
    #中分类名称
    m_category_name = scrapy.Field()
    #中分类URL
    m_category_url = scrapy.Field()
    #小分类名称
    s_category_name = scrapy.Field()
    #小分类URL
    s_category_url = scrapy.Field()

```

##### 4.2商品数据模型

- 商品数据模型类：用于存储商品信息（Product）
- 字段：
  - `product_category`：商品类别
  - `product_category_id`：商品类别ID
  - `product_sku_id`：商品ID
  - `product_name`：商品名称
  - `product_img_url`：商品图片URL
  - `product_book_info`：图书信息,作者，出版社
  - `product_option`：商品选项
  - `product_shop`：商品店铺
  - `product_comment：`商品评论数量
  - `product_ad`：商品促销
- `product_price`：商品价格
  
- 代码

```python
class Product(scrapy.Item):
    product_category = scrapy.Field() #商品类别
    product_category_id = scrapy.Field() #商品类别ID
    product_sku_id = scrapy.Field() #商品ID
    product_name = scrapy.Field() #商品名称
    product_img_url = scrapy.Field() #商品图片URL
    product_book_info = scrapy.Field() #图书信息, 作者，出版社
    product_option = scrapy.Field() #商品选项
    product_shop = scrapy.Field() #商品店铺
    product_comment = scrapy.Field() #商品评论数量
    product_ad = scrapy.Field() #商品促销
    product_price = scrapy.Field() #商品价格
```



#### 5. 商品分类爬虫

- 目标：抓取各级分类信息
- 步骤：
  1. 分析页面，确认分类信息的URL
  2. 创建类别爬虫，抓取数据

#####  5.1 分析，分类信息的URL

- 目标：确认分类信息的URL

- 步骤：

  1. 进入到京东首页

  2. 邮件检查，打开开发者工具，搜索 `家用电器`

  3. 确认分类的URL

     >  https://dc.3.cn/category/get 

#####   5.2 创建爬虫，抓取数据

- 目标：抓取分类数据，交给引擎
- 步骤：
  1. 创建类别爬虫
  2. 指定起始URL
  3. 解析数据，交给引擎

######    5.2.1 创建爬虫

```
进入项目目录： cd mall_spider
创建爬虫： scrapy genspider category_spider jd.com
```

######  5.2.2 指定起始URL

```
修改起始URL： https://dc.3.cn/category/get
```

###### 5.2.3 解析数据，交给引擎

- 分析数据格式

![](.\images\2020-5-24-4.png)

- 三类数据格式：

  - `list.jd.com/list.html?cat=1713,3286`|科学与自然||0

    > 对应的url：`list.jd.com/list.html?cat=1713,3286` 本身就是个url

  - `13765-13866-13867`|游戏机||0

    > 对应的url： https://list.jd.com/list.html?cat=13765,13866,13867 
    >
    > url构造：https://list.jd.com/list.html?cat={}
    >
    > - 把 `-`替换成 `,`  然后填充到占位的地方

  - `1713-3287`|计算机与互联网||0

    >  https://channel.jd.com/1713-3287.html
    >
    > url构造： https://channel.jd.com/{}.html

- 代码：

```python
# -*- coding: utf-8 -*-
#jd_category.py
import scrapy
import json

from mall_spider.items import Category


class JdCategorySpider(scrapy.Spider):
    name = 'jd_category'
    allowed_domains = ['3.cn']
    start_urls = ['http://dc.3.cn/category/get']

    def parse(self, response):
        #pass
        # print(response.body.decode('GBK'))
        result = json.loads(response.body.decode('GBK'))
        datas = result['data']
        #遍历数据列表
        for data in datas:

            item = Category()

            b_category = data['s'][0]
            b_category_info = b_category['n']
            # print('大分类：{}'.format(b_category_info))
            item['b_category_name'], item['b_category_url'] = self.get_category_name_url(b_category_info)

            #中分类信息列表
            m_category_list = b_category['s']
            #遍历中分类列表
            for m_category in m_category_list:
                #中分类信息
                m_category_info = m_category['n']
                # print('中分类：{}'.format(m_category_info))
                item['m_category_name'], item['m_category_url'] = self.get_category_name_url(m_category_info)

                #小分类数据列表
                s_category_list = m_category['s']
                for s_category in s_category_list:
                    s_category_info = s_category['n']
                    # print('小分类：{}'.format(s_category_info))
                    item['s_category_name'], item['s_category_url'] = self.get_category_name_url(s_category_info)
                    # print(item)
                    #把数据交给引擎
                    yield item

    def get_category_name_url(self, category_info):
        """
        根据分类信息，提取名称和URL
        :param category_info:  分类信息
        :return: 分类的名称和URL

        ① list.jd.com/list.html?cat=1713,3286|科学与自然||0
        对应的url：list.jd.com/list.html?cat=1713,3286 本身就是个url

        ② 1713-3287|计算机与互联网||0
        https://channel.jd.com/1713-3287.html
        url构造： https://channel.jd.com/{}.html

        ③ 13765-13866-13867|游戏机||0
        对应的url： https://list.jd.com/list.html?cat=13765,13866,13867
         url构造：https://list.jd.com/list.html?cat={}
            把 `-`替换成 `,`  然后填充到占位的地方
        """

        category = category_info.split('|')
        category_url = category[0]    #分类URL
        category_name = category[1]   #分类名称

        #处理第一类URL
        if category_url.count('jd.com') == 1:
            #URL进行补全
            category_url = 'https://' + category_url
        elif category_url.count('-') == 1:
            #1713-3287|计算机与互联网||0
            category_url = 'https://channel.jd.com/{}.html'.format(category_url)
        else:
            #13765-13866-13867|游戏机||0
            #把url中的 '-' 替换为 ','
            category_url = category_url.replace('-', ',')
            #补全url:
            category_url = 'https://list.jd.com/list.html?cat={}'.format(category_url)

        #返回类别的名称和URL
        return category_name, category_url
```



#### 6. 保存分类信息

- 目标：把分类信息保存到MongoDB
- 步骤：
  - 实现保存分类的Pipeline类
  - 在settings.py开启，类别的Pipeline



##### 	6.1 实现分类保存的Pipline类

- 步骤：
  1. open_spider 方法中，链接MongoDB数据库，获取要操作的集合
  2. process_item 方法中，向MongoDB中插入类别数据
  3. close_spider 方法中，关闭MongoDB的链接
- 代码：

```python
# -*- coding: utf-8 -*-
#pipelines.py

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html

"""
6.1 实现分类保存的Pipline类
  1. open_spider 方法中，链接MongoDB数据库，获取要操作的集合
  2. process_item 方法中，向MongoDB中插入类别数据
  3. close_spider 方法中，关闭MongoDB的链接
"""
from mall_spider.spiders.jd_category import JdCategorySpider
from pymongo import MongoClient
from mall_spider.settings import MONGODB_URL


class CategoryPipeline:

    def open_spider(self, spider):
        """当爬虫启动的时候执行"""
        if isinstance(spider, JdCategorySpider):
            #1. open_spider 方法中，链接MongoDB数据库，获取要操作的集合
            self.client = MongoClient(MONGODB_URL)
            self.collections = self.client['jd']['category']



    def process_item(self, item, spider):
        #  2. process_item 方法中，向MongoDB中插入类别数据
        if isinstance(spider, JdCategorySpider):
            self.collections.insert_one(dict(item))

        return item

    def close_spider(self, spider):
        #  3. close_spider 方法中，关闭MongoDB的链接
        if isinstance(spider, JdCategorySpider):
            self.client.close()

```

```python
#settings.py
#配置MongoDB的URL
MONGODB_URL = 'mongodb://127.0.0.1:27017'
```



##### 6.2 在 `settings.py` 开启，类别的Pipeline

```python
ITEM_PIPELINES = {
    'mall_spider.pipelines.CategoryPipeline': 300,
}
```



#### 7. 实现商品爬虫

- 总体设计：

  1. 把MongoDB中存储的分类信息，放到redis_key指定列表中
  2. 支持分布式爬虫，当然也可以在一台电脑上运行多次，以启动多个进程，充分使用CPU的多核
  3. 所以这里的爬虫，先从一个分类开始抓就可以了，以后再改造为分布式

  ![](.\images\2020-5-24-3.png)

- 目标：抓取商品数据

- 步骤：

  1. 分析，确定数据所在的URL
  2. 代码实现（核心）
  3. 商品爬虫实现分布式

#####  7.1  分析，确定数据所在的URL

- 列表页
  - 抓取商品`skuid`，实现翻页，确定翻页的URL
  - 获取商品的基本信息、通过手机抓包（APP），确定URL
  - PC详情页面，确定商品的促销信息的URL
  - PC详情页面，确定评论信息的URL
  - PC详情页面，确定商品价格信息的URL



#####  7.2 代码实现

- 步骤：
  1. 重写start_requests方法，根据分类信息构建列表页的请求
  
  2. 解析列表页，提取商品的`skuid`，构建商品基本信息请求，实现翻页
  
     1. 确定商品基本的信息请求（用Charles抓包）
  
      1. URL:  https://cdnware.m.jd.com/c1/skuDetail/apple/7.3.0/100005224262.json 
        2. 请求方法：Get
        3. 参数/数据：`100005224262`商品skuid
  
     2. 解析列表页，提取商品的skuid
  
        `//li[contains(@class,"gl-item")]/@data-sku`
  
     3. 构建商品基本的信息请求
  
  3. 解析商品评价信息，构建价格信息的请求
  
     1. 解析商品基本信息
  
        1. `product_category_id`：商品类别ID
        2. `product_name`：商品名称
        3. `product_img_url`：商品图片URL
        4. `product_book_info`：图书信息,作者，出版社
        5. `product_option`：商品选项
        6. `product_shop`：商品店铺
  
     2. 构建商品出校信息的请求
  
        1. 准备促销信息的请求
  
           1. URL ： https://cd.jd.com/promotion/v2?skuId=100008828920&area=2_2834_51982_0&cat=737%2C794%2C798 
  
           2. 请求方法：Get
  
           3. 参数/数据：
  
              >skuId=100008828920  #商品SKUID
              >
              >area=2_2834_51982_0  #区域固定值
              >
              >cat=737%2C794%2C798  #类别
  
  解析价格信息  
  
- 代码：